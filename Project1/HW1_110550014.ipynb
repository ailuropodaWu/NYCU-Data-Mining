{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train data and divide it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_train_data = pd.read_csv('./dataset/train.csv')\n",
    "row_train_data.drop(columns='Location', inplace=True)\n",
    "for idx, row in row_train_data.iterrows():\n",
    "    row_train_data.loc[idx] = [s.replace(' ', '') for s in row.values]\n",
    "row_train_data['Month'] = row_train_data['Date'].apply(lambda x: x[:x.find('/')])\n",
    "null = 0\n",
    "\n",
    "item_list = list(row_train_data.groupby('ItemName').groups.keys())\n",
    "item_train_data = {}\n",
    "for item in item_list:\n",
    "    item_row_data = row_train_data[row_train_data['ItemName'] == item]\n",
    "    train_list = []\n",
    "    for _, row in item_row_data.iterrows():\n",
    "        row = row.values[2:]\n",
    "        for i in range(len(row)):\n",
    "            try:\n",
    "                row[i] = float(row[i])\n",
    "            except:\n",
    "                null+=1\n",
    "                end = False\n",
    "                l = i\n",
    "                r = i\n",
    "                while not end:\n",
    "                    l = max(0, l - 1)\n",
    "                    r = min(8, r + 1)\n",
    "                    try:\n",
    "                        row[i] = (r - i) / (r - l) * float(row[l]) + (i - l) / (r - l) * float(row[r])\n",
    "                        end = True\n",
    "                    except:\n",
    "                        row[i] = np.nan\n",
    "                    if l == 0 and r == 8:\n",
    "                        break\n",
    "        train_list.extend(row[:-1])\n",
    "    train_list = np.array(train_list).reshape(12, 480)\n",
    "    item_train_data[item] = train_list\n",
    "\n",
    "\n",
    "random_seed = 10\n",
    "# ratio of training data\n",
    "train_ratio = 0.85\n",
    "train_sample = int(470 * train_ratio)\n",
    "np.random.seed(random_seed)\n",
    "sample_idx = np.array([np.random.permutation(470) for _ in range(12)])\n",
    "# indexes of training data and validation data\n",
    "train_idx, val_idx = sample_idx[:, :train_sample], sample_idx[:, train_sample:]\n",
    "\n",
    "header = ['DataIndex', 'Month', 'Item']\n",
    "header.extend([f'{i}' for i in range(10)])\n",
    "\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "dropna = False\n",
    "\n",
    "label = []\n",
    "data_index = 0\n",
    "for month in range(12):\n",
    "    for i in train_idx[month]:\n",
    "        single_data = []\n",
    "        for item in item_list:\n",
    "            row = [data_index, month+1, item]\n",
    "            row.extend(item_train_data[item][month][i:i+10])\n",
    "            if dropna and None in row:\n",
    "                single_data.clear()\n",
    "                break\n",
    "            else:\n",
    "                single_data.append(row)\n",
    "        if single_data:\n",
    "            train_data.extend(single_data)\n",
    "            label.extend([single_data[9][-1]] * 18)\n",
    "            data_index += 1\n",
    "train_data = pd.DataFrame(train_data, columns=header)\n",
    "train_data['Label'] = label\n",
    "train_data.drop(columns=['9', 'Month'], inplace=True)\n",
    "print(train_data)\n",
    "\n",
    "label = []\n",
    "data_index = 0\n",
    "for month in range(12):\n",
    "    for i in val_idx[month]:\n",
    "        single_data = []\n",
    "        for item in item_list:\n",
    "            row = [data_index, month+1, item]\n",
    "            row.extend(item_train_data[item][month][i:i+10])\n",
    "            if dropna and None in row:\n",
    "                single_data.clear()\n",
    "                break\n",
    "            else:\n",
    "                single_data.append(row)\n",
    "        if single_data:\n",
    "            val_data.extend(single_data)\n",
    "            label.extend([single_data[9][-1]] * 18)\n",
    "            data_index += 1\n",
    "val_data = pd.DataFrame(val_data, columns=header)\n",
    "val_data['Label'] = label\n",
    "val_data.drop(columns=['9', 'Month'], inplace=True)\n",
    "print(val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./dataset/test.csv', header=None)\n",
    "test_data.columns = train_data.columns[:-1]\n",
    "for idx, row in test_data.iterrows():\n",
    "    row = [s.replace(' ', '') for s in row.values]\n",
    "    for i in range(2, len(row)):\n",
    "        try:\n",
    "            row[i] = float(row[i])\n",
    "        except:\n",
    "            end = False\n",
    "            l = i\n",
    "            r = i\n",
    "            while not end:\n",
    "                l = max(0, l - 1)\n",
    "                r = min(8, r + 1)\n",
    "                try:\n",
    "                    row[i] = (r - i) / (r - l) * float(row[l]) + (i - l) / (r - l) * float(row[r])\n",
    "                    end = True\n",
    "                except:\n",
    "                    row[i] = np.nan\n",
    "                if l == 0 and r == 8:\n",
    "                    break\n",
    "    test_data.loc[idx] = row\n",
    "test_data['DataIndex'] = test_data['DataIndex'].apply(lambda x: int(x.replace('index_', '')))\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function declaration and implement Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.closed_form_weights = None\n",
    "        self.closed_form_intercept = None\n",
    "        self.gradient_descent_weights = None\n",
    "        self.gradient_descent_intercept = None\n",
    "        self.loss = []\n",
    "        \n",
    "    def closed_form_fit(self, X, y):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        X_T = X.T\n",
    "        theta = np.linalg.inv(X_T.dot(X)).dot(X_T).dot(y)\n",
    "        self.closed_form_intercept, self.closed_form_weights = theta[0], theta[1:]\n",
    "\n",
    "    def gradient_descent_fit(self, X, y, lr, epochs, l1=None, l2=None):\n",
    "        np.random.seed(random_seed)\n",
    "        initial_theta = np.random.rand(X.shape[1] + 1) \n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        theta = initial_theta.copy()\n",
    "        gradient = np.zeros(X_b.shape[1])\n",
    "        self.loss.clear()\n",
    "        for iteration in range(epochs):\n",
    "            y_pred = X_b.dot(theta)\n",
    "            error = y_pred - y\n",
    "            mse = (1 / X.shape[0]) * np.sum(np.square(error))\n",
    "            if len(self.loss) > 0 and self.loss[-1] - mse < 1e-5:\n",
    "                print(f\"Early break w/ iterations: {iteration}\")\n",
    "                break\n",
    "            self.loss.append(mse)\n",
    "            l1_penalty = np.array([l1] * theta.shape[0]) * np.abs(theta) if l1 else np.zeros(theta.shape[0])\n",
    "            l2_penalty = np.array([l2] * theta.shape[0]) * np.power(theta, 2) if l2 else np.zeros(theta.shape[0])\n",
    "            l1_penalty[0] = 0\n",
    "            l2_penalty[0] = 0\n",
    "            \n",
    "            gradient = (2 / X.shape[0]) * X_b.T.dot(error) + l1_penalty + l2_penalty\n",
    "            theta -= lr * gradient\n",
    "        self.gradient_descent_intercept, self.gradient_descent_weights = theta[0], theta[1:]\n",
    "        \n",
    "\n",
    "    def get_rmse_loss(self, prediction, ground_truth):\n",
    "        return np.sqrt(sum(np.square(prediction - ground_truth)) / prediction.shape[0])\n",
    "\n",
    "    def closed_form_predict(self, X):\n",
    "        return X.dot(self.closed_form_weights) + self.closed_form_intercept\n",
    "\n",
    "    def gradient_descent_predict(self, X):\n",
    "        return X.dot(self.gradient_descent_weights) + self.gradient_descent_intercept\n",
    "    \n",
    "    def closed_form_evaluate(self, X, y):\n",
    "        return self.get_rmse_loss(self.closed_form_predict(X), y)\n",
    "\n",
    "    def gradient_descent_evaluate(self, X, y):\n",
    "        return self.get_rmse_loss(self.gradient_descent_predict(X), y)\n",
    "        \n",
    "    def plot_learning_curve(self):\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(self.loss, marker = 'o', linestyle = '-')\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('RMSE Loss')\n",
    "        plt.show()\n",
    "\n",
    "# Used to show the correlation between features and the labels\n",
    "\n",
    "def feature_corr(item, option: str):\n",
    "    feature_data = train_data[train_data['Item'] == item]\n",
    "    feature_avg = []\n",
    "    for _, row in feature_data.iterrows():\n",
    "        avg = None\n",
    "        if option == 'last':\n",
    "            avg = row.values[2:-1][-1]\n",
    "        elif option == 'mean':\n",
    "            avg = np.mean(row.values[2:-1])\n",
    "        elif option == 'std':\n",
    "            avg = np.std(row.values[2:-1])\n",
    "        elif option == 'median':\n",
    "            avg = np.median(row.values[2:-1])\n",
    "        elif option == 'max':\n",
    "            avg = np.max(row.values[2:-1])\n",
    "        elif option == 'min':\n",
    "            avg = np.min(row.values[2:-1])\n",
    "        elif option in [f'{v}' for v in range(9)]:\n",
    "            avg = row.values[2:-1][int(option)]\n",
    "        else:\n",
    "            return\n",
    "        feature_avg.append(avg)\n",
    "    plot_handle = np.array([feature_avg, list(feature_data['Label'])])\n",
    "\n",
    "    plt.figure(figsize=(3, 2))\n",
    "    plt.scatter(plot_handle[0], plot_handle[1])\n",
    "    plt.xlabel(f'{item} {option}')\n",
    "    plt.ylabel('Label')\n",
    "    plt.show()\n",
    "\n",
    "# Extract the features from data\n",
    "\n",
    "def feature_extract(data, train_flag=True):\n",
    "    global feature_list, feature_num, nan\n",
    "    X = np.zeros((data['DataIndex'][len(data)-1] + 1, feature_num + 1 if train_flag else feature_num))\n",
    "    features = []\n",
    "    current_idx = 0\n",
    "    for i, row in data.iterrows():\n",
    "        if row['DataIndex'] == current_idx + 1:\n",
    "            if train_flag: features.append(data['Label'][i-1])\n",
    "            for j in range(len(features)):\n",
    "                try: int(features[j])\n",
    "                except: features[j] = 0\n",
    "            X[current_idx, :] = np.array(features)\n",
    "            features.clear()\n",
    "            current_idx += 1\n",
    "        if row['Item'] in feature_list.keys():\n",
    "            for option in feature_list[row['Item']]:\n",
    "                values = row.values[2:-1]\n",
    "                values_dropna = np.array([v for v in values if not np.isnan(v)])\n",
    "                try:\n",
    "                    if len(values_dropna) <= 0:\n",
    "                        feature = 0\n",
    "                    elif option == 'mean':\n",
    "                        feature = np.mean(values_dropna)\n",
    "                    elif option == 'std':\n",
    "                        feature = np.std(values_dropna)\n",
    "                    elif option == 'median':\n",
    "                        feature = np.median(values_dropna)\n",
    "                    elif option == 'max':\n",
    "                        feature = np.max(values_dropna)\n",
    "                    elif option == 'min':\n",
    "                        feature = np.min(values_dropna)\n",
    "                    else:\n",
    "                        square = False\n",
    "                        log = False\n",
    "                        if '-square' in option:\n",
    "                            option = option.replace('-square', '')\n",
    "                            square = True\n",
    "                        elif '-log' in option:\n",
    "                            option = option.replace('-log', '')\n",
    "                            log = True\n",
    "                        feature = float(row[option])\n",
    "                        if np.isnan(feature):\n",
    "                            feature = 0\n",
    "                        elif square:\n",
    "                            feature *= feature\n",
    "                        elif log:\n",
    "                            feature = np.log(feature)\n",
    "                except:\n",
    "                    feature = 0\n",
    "                features.append(feature)  \n",
    "    return X\n",
    "\n",
    "def train_test_split(X, train_ratio=0.8):\n",
    "    global random_seed\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    train_size = int(X.shape[0] * train_ratio)\n",
    "    train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "    return X[train_idx], X[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in item_list:\n",
    "#     feature_corr(item, '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide the features\n",
    "# options:\n",
    "#           0-8[-square][-log]: time based features and do some mathematical processing\n",
    "#           mean, std, median, max, min: statistics from the data without nan value \n",
    "feature_list = {'PM2.5': ['all', 'mean', '8-square'],\n",
    "                'PM10': ['all', 'mean', '8-square'],\n",
    "                'NO': ['8', 'mean'],\n",
    "                'CO': ['8', 'mean'],\n",
    "                'THC': ['8', 'mean'],\n",
    "                'CH4': ['8', 'mean'],\n",
    "                'WS_HR': ['all', 'mean', 'max', 'min'],\n",
    "                'WD_HR': ['all', 'median', 'max', 'min'],\n",
    "                }\n",
    "verbose = False\n",
    "for key, values in feature_list.items():\n",
    "    if 'all' in values:\n",
    "        feature_list[key].extend([f'{v}' for v in range(9)])\n",
    "        feature_list[key].remove('all')\n",
    "    for value in values:\n",
    "        verbose and feature_corr(key, value)\n",
    "ls = []\n",
    "for x in feature_list.values():\n",
    "    ls.extend([v for v in x])\n",
    "feature_num = len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_extract(train_data)\n",
    "val = feature_extract(val_data)\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the result of different size of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "split_ratio_list = [float(x + 1) / 10 for x in range(1, 10)]\n",
    "for split_ratio in split_ratio_list:\n",
    "    split_train, _ = train_test_split(train, split_ratio)\n",
    "    y_train, y_val = split_train[:, -1], val[:, -1]\n",
    "    X_train, X_val = split_train[:, :-1],val[:, :-1]\n",
    "    LRGD = LinearRegression()\n",
    "    LRGD.gradient_descent_fit(X_train, y_train, 1e-7, int(1e6))\n",
    "    result.append(LRGD.gradient_descent_evaluate(X_val, y_val))\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(split_ratio_list, result, marker = 'o', linestyle = '-')\n",
    "plt.title('Result')\n",
    "plt.xlabel('Train data ratio')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the result of different L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "l2_list = [0, 1, 5, 10, 20]\n",
    "for l2 in l2_list:\n",
    "    split_train, _ = train_test_split(train, 1)\n",
    "    y_train, y_val = split_train[:, -1], val[:, -1]\n",
    "    X_train, X_val = split_train[:, :-1],val[:, :-1]\n",
    "    LRGD = LinearRegression()\n",
    "    LRGD.gradient_descent_fit(X_train, y_train, 1e-7, int(1e6), l2=l2)\n",
    "    result.append(LRGD.gradient_descent_evaluate(X_val, y_val))\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(l2_list, result, marker = 'o', linestyle = '-')\n",
    "plt.title('Result')\n",
    "plt.xlabel('L2 Penalty')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train, _ = train_test_split(train, 1)\n",
    "y_train, y_val = split_train[:, -1], val[:, -1]\n",
    "X_train, X_val = split_train[:, :-1], val[:, :-1]\n",
    "LR = LinearRegression()\n",
    "LR.closed_form_fit(X_train, y_train)\n",
    "print(LR.closed_form_evaluate(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRGD = LinearRegression()\n",
    "LRGD.gradient_descent_fit(X_train, y_train, lr=1e-7, epochs=int(1e6), l2=5)\n",
    "LRGD.plot_learning_curve()\n",
    "print(LRGD.gradient_descent_evaluate(X_val, y_val))\n",
    "print(LRGD.gradient_descent_weights, LRGD.gradient_descent_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = feature_extract(test_data, False)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = LR.closed_form_predict(X_test)\n",
    "prediction = [[f'index_{i}', prediction[i]] for i in range(len(prediction))]\n",
    "prediction = pd.DataFrame(prediction, columns=['Index', 'answer'])\n",
    "prediction.to_csv('./prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = LRGD.gradient_descent_predict(X_test)\n",
    "prediction = [[f'index_{i}', prediction[i]] for i in range(len(prediction))]\n",
    "prediction = pd.DataFrame(prediction, columns=['Index', 'answer'])\n",
    "prediction.to_csv('./prediction_gd.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
